{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table(urls[0], header=None)\n",
    "df_valid = pd.read_table(urls[1], header=None)\n",
    "df_test = pd.read_table(urls[2], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.vocab = []\n",
    "    \n",
    "    def readData(self, data, sep=None):\n",
    "        self.corpus = [x.split(sep) for x in data]\n",
    "        \n",
    "        self.vocab = set([y for x in self.corpus for y in x])\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.createIndices()\n",
    "    \n",
    "    def createIndices(self):\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        \n",
    "        # error checks\n",
    "        assert len(self.word2idx) == self.vocab_size\n",
    "        assert len(self.idx2word) == self.vocab_size\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{0} sentences in data, consisting of {1} unique words\".format(len(self.corpus), self.nunique)\n",
    "    \n",
    "    def generate_pairs(self, window_size):\n",
    "        idx_pairs = []\n",
    "\n",
    "        for sentence in self.corpus:\n",
    "            indices = [self.word2idx[word] for word in sentence]\n",
    "            # for each word, threated as center word\n",
    "            for center_word_pos in range(len(indices)):\n",
    "                # for each window position\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = indices[context_word_pos]\n",
    "                    idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "        self.idx_pairs = np.array(idx_pairs)  \n",
    "        \n",
    "    def wordFrequencies(self, n=10, update=False):\n",
    "        \n",
    "        if not hasattr(self, \"sorted_counts\") or update:\n",
    "            flattened = [y for x in self.corpus for y in x]\n",
    "            self.nwords = len(flattened)\n",
    "            counts = Counter(flattened)\n",
    "\n",
    "            self.sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            \n",
    "            self.word2freq = {w: f/self.nwords for (w, f) in counts.items()}\n",
    "        \n",
    "        print(\"Top {0} most common words\".format(n))\n",
    "        for i in range(n):\n",
    "            print(self.sorted_counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader()\n",
    "train_data.readData(data=df_train[0])\n",
    "train_data.generate_pairs(window_size=2)\n",
    "\n",
    "valid_data = DataLoader()\n",
    "valid_data.readData(data=df_valid[0])\n",
    "valid_data.generate_pairs(window_size=2)\n",
    "\n",
    "test_data = DataLoader()\n",
    "test_data.readData(data=df_test[0])\n",
    "test_data.generate_pairs(window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words\n",
      "('the', 50770)\n",
      "('<unk>', 45020)\n",
      "('N', 32481)\n",
      "('of', 24400)\n",
      "('to', 23638)\n",
      "('a', 21196)\n",
      "('in', 18000)\n",
      "('and', 17474)\n",
      "(\"'s\", 9784)\n",
      "('that', 8931)\n"
     ]
    }
   ],
   "source": [
    "train_data.wordFrequencies(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram\n",
    "\n",
    "Objective of skip-gram model: figure out word representations that are useful for predicting the surrounding words in a sentence. \n",
    "\n",
    "Given a sequence of words for training, $w_1,w_2,w_3,...,w_T$, the objective is then to maximize the average log probability over the training context $c$:\n",
    "$$\\frac{1}{T}\\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq0} \\log p(w_{t+j}|w_t)$$\n",
    "\n",
    "$p(w_{t+j}|w_t)$ is defined by the softmax function $$p(w_{O}|w_I)=\\frac{\\exp\\big(v_{w_O}'^\\intercal v_{w_I}\\big)}{\\sum_{w=1}^W\\exp(v_w'^\\intercal v_{w_I})}$$\n",
    "\n",
    "Assumtions:\n",
    "* context windows are symmetrical with a value $x\\in \\R $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (embeddings): Embedding(9999, 1)\n",
      "  (hidden): Linear(in_features=1, out_features=30, bias=True)\n",
      "  (output): Linear(in_features=30, out_features=9999, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, h_dim, nwords):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            vocab_size: number of vocab\n",
    "            embedding_dim: embedding dimensions\n",
    "            h_dim: output dimension of hidden layer\n",
    "            nwords: number of unique words that can be predicted\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(in_features=embedding_dim, out_features=h_dim)\n",
    "        \n",
    "        # output layer        \n",
    "        self.output = nn.Linear(in_features=h_dim, out_features=nwords)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            x:\n",
    "        \"\"\"\n",
    "        \n",
    "        # store results in dictionary\n",
    "        out = {}\n",
    "        \n",
    "        # get embeddings for input x\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # run through hidden layer\n",
    "        x = self.hidden(x)\n",
    "        \n",
    "        # Softmax on output of the hidden layer\n",
    "        out['out'] = nn.functional.softmax(self.output(x), dim=1)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "net = SkipGram(vocab_size=train_data.vocab_size, embedding_dim=1, h_dim=30, nwords=train_data.vocab_size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input(word_idx, vocab_size):\n",
    "    \"\"\"\n",
    "    Quick function for converting input to tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    x = torch.zeros(vocab_size).long()\n",
    "    x[word_idx] = 1.0\n",
    "    \n",
    "    return x\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.from_numpy(train_data.idx_pairs[:,0]), torch.from_numpy(train_data.idx_pairs[:,1]))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "valid = data_utils.TensorDataset(torch.from_numpy(valid_data.idx_pairs[:,0]), torch.from_numpy(valid_data.idx_pairs[:,1]))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.210232734680176 0.0\n"
     ]
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))\n",
    "output = net(data.long())\n",
    "loss = criterion(output['out'], target.long())\n",
    "acc = accuracy(output['out'], target.long())\n",
    "\n",
    "print(loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the network\n",
    "\n",
    "should implement batch sizes, as this is too slow :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 9.21, accs: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-52df5c38791c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 862\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1550\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[0;32m    973\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "train_loss, train_accs = [],  []\n",
    "valid_loss, valid_accs = [],  []\n",
    "\n",
    "eval_every = 2\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):    \n",
    "    for data, target in train_loader:        \n",
    "        x = data.long()\n",
    "        y_true = target.long()\n",
    "        \n",
    "        output = net(x)\n",
    "        \n",
    "        loss = criterion(output['out'], y_true)\n",
    "        acc = accuracy(output['out'], y_true)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % eval_every == 0:\n",
    "        \n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        \n",
    "        for i, (val_data, val_target) in enumerate(valid_loader):\n",
    "            x = data.long()\n",
    "            y_true = target.long()\n",
    "            \n",
    "            output = net(x)\n",
    "            \n",
    "            loss = criterion(output['out'], y_true)\n",
    "            acc = accuracy(output['out'], y_true)\n",
    "            \n",
    "            val_losses += loss * valid_loader.batch_size\n",
    "            val_accs += acc * valid_loader.batch_size\n",
    "            val_lengths += valid_loader.batch_size\n",
    "            \n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        \n",
    "        print(\"Epoch: {}, train loss: {:.2f}, accs: {:.2f}\".format(epoch, np.mean(train_loss), np.mean(train_accs)))\n",
    "        print(\"Epoch: {}, valid loss: {:.2f}, accs: {:.2f}\".format(epoch, get_numpy, np.mean(train_accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- should we include subsampling!? probably :)\n",
    "- padding?\n",
    "\n",
    "# Good urls\n",
    "https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "https://github.com/deborausujono/word2vecpy/blob/master/word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
