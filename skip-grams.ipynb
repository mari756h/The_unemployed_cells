{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table(urls[0], header=None)\n",
    "df_valid = pd.read_table(urls[1], header=None)\n",
    "df_test = pd.read_table(urls[2], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.vocab = []\n",
    "    \n",
    "    def readData(self, data, sep=None):\n",
    "        self.corpus = [x.split(sep) for x in data]\n",
    "        \n",
    "        self.vocab = set([y for x in self.corpus for y in x])\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.createIndices()\n",
    "    \n",
    "    def createIndices(self):\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        \n",
    "        # error checks\n",
    "        assert len(self.word2idx) == self.vocab_size\n",
    "        assert len(self.idx2word) == self.vocab_size\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{0} sentences in data, consisting of {1} unique words\".format(len(self.corpus), self.nunique)\n",
    "    \n",
    "    def generate_pairs(self, window_size):\n",
    "        idx_pairs = []\n",
    "\n",
    "        for sentence in self.corpus:\n",
    "            indices = [self.word2idx[word] for word in sentence]\n",
    "            # for each word, threated as center word\n",
    "            for center_word_pos in range(len(indices)):\n",
    "                # for each window position\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = indices[context_word_pos]\n",
    "                    idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "        self.idx_pairs = np.array(idx_pairs)  \n",
    "        \n",
    "    def wordFrequencies(self, n=10, update=False):\n",
    "        \n",
    "        if not hasattr(self, \"sorted_counts\") or update:\n",
    "            flattened = [y for x in self.corpus for y in x]\n",
    "            self.nwords = len(flattened)\n",
    "            counts = Counter(flattened)\n",
    "\n",
    "            self.sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            \n",
    "            self.word2freq = {w: f/self.nwords for (w, f) in counts.items()}\n",
    "        \n",
    "        print(\"Top {0} most common words\".format(n))\n",
    "        for i in range(n):\n",
    "            print(self.sorted_counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader()\n",
    "train_data.readData(data=df_train[0])\n",
    "train_data.generate_pairs(window_size=1)\n",
    "\n",
    "valid_data = DataLoader()\n",
    "valid_data.readData(data=df_valid[0])\n",
    "valid_data.generate_pairs(window_size=1)\n",
    "\n",
    "test_data = DataLoader()\n",
    "test_data.readData(data=df_test[0])\n",
    "test_data.generate_pairs(window_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words\n",
      "('the', 50770)\n",
      "('<unk>', 45020)\n",
      "('N', 32481)\n",
      "('of', 24400)\n",
      "('to', 23638)\n",
      "('a', 21196)\n",
      "('in', 18000)\n",
      "('and', 17474)\n",
      "(\"'s\", 9784)\n",
      "('that', 8931)\n"
     ]
    }
   ],
   "source": [
    "train_data.wordFrequencies(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram\n",
    "\n",
    "Objective of skip-gram model: figure out word representations that are useful for predicting the surrounding words in a sentence. \n",
    "\n",
    "Given a sequence of words for training, $w_1,w_2,w_3,...,w_T$, the objective is then to maximize the average log probability over the training context $c$:\n",
    "$$\\frac{1}{T}\\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq0} \\log p(w_{t+j}|w_t)$$\n",
    "\n",
    "$p(w_{t+j}|w_t)$ is defined by the softmax function $$p(w_{O}|w_I)=\\frac{\\exp\\big(v_{w_O}'^\\intercal v_{w_I}\\big)}{\\sum_{w=1}^W\\exp(v_w'^\\intercal v_{w_I})}$$\n",
    "\n",
    "Assumtions:\n",
    "* context windows are symmetrical with a value $x\\in \\R $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (in_embeddings): Embedding(9999, 300, sparse=True)\n",
      "  (out_embeddings): Embedding(9999, 300, sparse=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            vocab_size: number of vocab\n",
    "            embedding_dim: embedding dimensions\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.in_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, sparse=True)\n",
    "        self.out_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, sparse=True)\n",
    "        \n",
    "        # hidden layer\n",
    "#         self.hidden = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        \n",
    "        # output layer        \n",
    "#         self.output = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x, yt):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            x: input data\n",
    "            yt: true y\n",
    "        \n",
    "        Return:\n",
    "            out: predictions of words, dim corresponds to vocab size\n",
    "        \"\"\"\n",
    "        \n",
    "        # get embeddings for input x\n",
    "        x_vec = self.in_embeddings(x)\n",
    "        \n",
    "        # get embeddings for output yt\n",
    "        yt_vec = self.out_embeddings(yt)\n",
    "\n",
    "        # calculate output\n",
    "        out = torch.mul(x_vec, yt_vec)\n",
    "        \n",
    "        loss = torch.sum(-F.logsigmoid(out), -1)\n",
    "        loss = torch.mean(loss)\n",
    "        \n",
    "        return out, loss\n",
    "        \n",
    "        \n",
    "net = SkipGram(vocab_size=train_data.vocab_size, embedding_dim=300)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() and args.gpu_id > -1\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(args.gpu_id)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.from_numpy(train_data.idx_pairs[:,0]), torch.from_numpy(train_data.idx_pairs[:,1]))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "valid = data_utils.TensorDataset(torch.from_numpy(valid_data.idx_pairs[:,0]), torch.from_numpy(valid_data.idx_pairs[:,1]))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238.08462524414062\n"
     ]
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))\n",
    "\n",
    "if use_cuda:\n",
    "    data = data.cuda()\n",
    "    target = target.cuda()\n",
    "\n",
    "output, loss = net(x = data.long(), yt=target.long())\n",
    "# loss = criterion(output, target.long())\n",
    "# acc = accuracy(output, target.long())\n",
    "\n",
    "print(loss.item())#, acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the network\n",
    "\n",
    "should implement batch sizes, as this is too slow :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iteration 1/26421, loss: 0.476\n",
      "Epoch 1, iteration 500/26421, loss: 237.557\n",
      "Epoch 1, iteration 999/26421, loss: 236.343\n",
      "Epoch 1, iteration 1498/26421, loss: 235.263\n",
      "Epoch 1, iteration 1997/26421, loss: 234.242\n",
      "Epoch 1, iteration 2496/26421, loss: 233.274\n",
      "Epoch 1, iteration 2995/26421, loss: 232.378\n",
      "Epoch 1, iteration 3494/26421, loss: 231.922\n",
      "Epoch 1, iteration 3993/26421, loss: 231.097\n",
      "Epoch 1, iteration 4492/26421, loss: 230.433\n",
      "Epoch 1, iteration 4991/26421, loss: 229.877\n",
      "Epoch 1, iteration 5490/26421, loss: 229.292\n",
      "Epoch 1, iteration 5989/26421, loss: 228.844\n",
      "Epoch 1, iteration 6488/26421, loss: 228.487\n",
      "Epoch 1, iteration 6987/26421, loss: 227.869\n",
      "Epoch 1, iteration 7486/26421, loss: 227.640\n",
      "Epoch 1, iteration 7985/26421, loss: 227.081\n",
      "Epoch 1, iteration 8484/26421, loss: 226.834\n",
      "Epoch 1, iteration 8983/26421, loss: 226.510\n",
      "Epoch 1, iteration 9482/26421, loss: 226.218\n",
      "Epoch 1, iteration 9981/26421, loss: 225.813\n",
      "Epoch 1, iteration 10480/26421, loss: 225.557\n",
      "Epoch 1, iteration 10979/26421, loss: 225.310\n",
      "Epoch 1, iteration 11478/26421, loss: 225.166\n",
      "Epoch 1, iteration 11977/26421, loss: 224.851\n",
      "Epoch 1, iteration 12476/26421, loss: 224.744\n",
      "Epoch 1, iteration 12975/26421, loss: 224.520\n",
      "Epoch 1, iteration 13474/26421, loss: 224.203\n",
      "Epoch 1, iteration 13973/26421, loss: 223.885\n",
      "Epoch 1, iteration 14472/26421, loss: 223.647\n",
      "Epoch 1, iteration 14971/26421, loss: 223.597\n",
      "Epoch 1, iteration 15470/26421, loss: 223.286\n",
      "Epoch 1, iteration 15969/26421, loss: 222.998\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "log_every = 499\n",
    "N = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    net.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # check if cuda is available\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        output, loss = net(inputs.long(), labels.long())\n",
    "#         loss = criterion(output, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % log_every == 0:\n",
    "            print(\"Epoch {}, iteration {}/{}, loss: {:.3f}\".format(epoch+1, i+1, N, running_loss / log_every))\n",
    "            \n",
    "            running_loss = 0.0  \n",
    "    \n",
    "    net.eval()\n",
    "    val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "    for j, (inputs, labels) in enumerate(valid_loader):\n",
    "        # check if cuda is available\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        # forward + backward\n",
    "        output, loss = net(inputs.long(), labels.long())\n",
    "#         loss = criterion(output, labels.long())\n",
    "        \n",
    "        val_losses += loss.item() * valid_loader.batch_size\n",
    "        val_accs += accuracy(output, labels.long()).item() * valid_loader.batch_size\n",
    "        val_lengths += valid_loader.batch_size\n",
    "        \n",
    "    val_losses /= val_lengths\n",
    "    val_accs /= val_lengths\n",
    "    \n",
    "    print(\"\\nValidation loss: {:.3f}, accuracy: {:.3f}\".format(val_losses, val_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 9.21, accs: 0.00\n",
      "Epoch: 0, valid loss: 9.21, accs: 0.00\n",
      "Epoch: 1, train loss: 9.21, accs: 0.00\n",
      "Epoch: 1, valid loss: 9.21, accs: 0.00\n",
      "Epoch: 2, train loss: 9.21, accs: 0.00\n",
      "Epoch: 2, valid loss: 9.21, accs: 0.00\n",
      "Epoch: 3, train loss: 9.21, accs: 0.00\n",
      "Epoch: 3, valid loss: 9.21, accs: 0.00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "train_loss, train_accs = [],  []\n",
    "valid_loss, valid_accs = [],  []\n",
    "\n",
    "eval_every = 1\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):    \n",
    "    for data, target in train_loader:        \n",
    "        x = data.long()\n",
    "        y_true = target.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net(x)\n",
    "        \n",
    "        loss = criterion(output['out'], y_true)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = accuracy(output['out'], y_true)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "        \n",
    "        \n",
    "    if epoch % eval_every == 0:\n",
    "        \n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        \n",
    "        for i, (val_data, val_target) in enumerate(valid_loader):\n",
    "            x = data.long()\n",
    "            y_true = target.long()\n",
    "            \n",
    "            output = net(x)\n",
    "            \n",
    "            loss = criterion(output['out'], y_true)\n",
    "            acc = accuracy(output['out'], y_true)\n",
    "            \n",
    "            val_losses += loss * valid_loader.batch_size\n",
    "            val_accs += acc * valid_loader.batch_size\n",
    "            val_lengths += valid_loader.batch_size\n",
    "            \n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        \n",
    "        print(\"Epoch: {}, train loss: {:.2f}, accs: {:.2f}\".format(epoch, np.mean(train_loss), np.mean(train_accs)))\n",
    "        print(\"Epoch: {}, valid loss: {:.2f}, accs: {:.2f}\".format(epoch, get_numpy(val_losses), get_numpy(val_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- should we include subsampling!? probably :)\n",
    "- padding?\n",
    "\n",
    "# Good urls\n",
    "https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "https://github.com/deborausujono/word2vecpy/blob/master/word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
