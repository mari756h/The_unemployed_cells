{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table(urls[0], header=None)\n",
    "df_valid = pd.read_table(urls[1], header=None)\n",
    "df_test = pd.read_table(urls[2], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.vocab = []\n",
    "    \n",
    "    def readData(self, data, sep=None):\n",
    "        self.corpus = [x.split(sep) for x in data]\n",
    "        \n",
    "        self.vocab = set([y for x in self.corpus for y in x])\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.createIndices()\n",
    "    \n",
    "    def createIndices(self):\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        \n",
    "        # error checks\n",
    "        assert len(self.word2idx) == self.vocab_size\n",
    "        assert len(self.idx2word) == self.vocab_size\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{0} sentences in data, consisting of {1} unique words\".format(len(self.corpus), self.nunique)\n",
    "    \n",
    "    def generate_pairs(self, window_size):\n",
    "        idx_pairs = []\n",
    "\n",
    "        for sentence in self.corpus:\n",
    "            indices = [self.word2idx[word] for word in sentence]\n",
    "            # for each word, threated as center word\n",
    "            for center_word_pos in range(len(indices)):\n",
    "                # for each window position\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = indices[context_word_pos]\n",
    "                    idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "        self.idx_pairs = np.array(idx_pairs)  \n",
    "        \n",
    "    def wordFrequencies(self, n=10, update=False):\n",
    "        \n",
    "        if not hasattr(self, \"sorted_counts\") or update:\n",
    "            flattened = [y for x in self.corpus for y in x]\n",
    "            self.nwords = len(flattened)\n",
    "            counts = Counter(flattened)\n",
    "\n",
    "            self.sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            \n",
    "            self.word2freq = {w: f/self.nwords for (w, f) in counts.items()}\n",
    "        \n",
    "        print(\"Top {0} most common words\".format(n))\n",
    "        for i in range(n):\n",
    "            print(self.sorted_counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader()\n",
    "train_data.readData(data=df_train[0])\n",
    "train_data.generate_pairs(window_size=2)\n",
    "\n",
    "valid_data = DataLoader()\n",
    "valid_data.readData(data=df_valid[0])\n",
    "valid_data.generate_pairs(window_size=2)\n",
    "\n",
    "test_data = DataLoader()\n",
    "test_data.readData(data=df_test[0])\n",
    "test_data.generate_pairs(window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words\n",
      "('the', 50770)\n",
      "('<unk>', 45020)\n",
      "('N', 32481)\n",
      "('of', 24400)\n",
      "('to', 23638)\n",
      "('a', 21196)\n",
      "('in', 18000)\n",
      "('and', 17474)\n",
      "(\"'s\", 9784)\n",
      "('that', 8931)\n"
     ]
    }
   ],
   "source": [
    "train_data.wordFrequencies(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram\n",
    "\n",
    "Objective of skip-gram model: figure out word representations that are useful for predicting the surrounding words in a sentence. \n",
    "\n",
    "Given a sequence of words for training, $w_1,w_2,w_3,...,w_T$, the objective is then to maximize the average log probability over the training context $c$:\n",
    "$$\\frac{1}{T}\\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq0} \\log p(w_{t+j}|w_t)$$\n",
    "\n",
    "$p(w_{t+j}|w_t)$ is defined by the softmax function $$p(w_{O}|w_I)=\\frac{\\exp\\big(v_{w_O}'^\\intercal v_{w_I}\\big)}{\\sum_{w=1}^W\\exp(v_w'^\\intercal v_{w_I})}$$\n",
    "\n",
    "Assumtions:\n",
    "* context windows are symmetrical with a value $x\\in \\R $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (embeddings): Embedding(9999, 1)\n",
      "  (hidden): Linear(in_features=1, out_features=30, bias=True)\n",
      "  (output): Linear(in_features=30, out_features=9999, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, h_dim, nwords):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            vocab_size: number of vocab\n",
    "            embedding_dim: embedding dimensions\n",
    "            h_dim: output dimension of hidden layer\n",
    "            nwords: number of unique words that can be predicted\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(in_features=embedding_dim, out_features=h_dim)\n",
    "        \n",
    "        # output layer        \n",
    "        self.output = nn.Linear(in_features=h_dim, out_features=nwords)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            x: input data\n",
    "        \n",
    "        Return:\n",
    "            out: predictions of words, dim corresponds to vocab size\n",
    "        \"\"\"\n",
    "        \n",
    "        # store results in dictionary\n",
    "        out = {}\n",
    "        \n",
    "        # get embeddings for input x\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # run through hidden layer\n",
    "        x = self.hidden(x)\n",
    "        \n",
    "        # Softmax on output of the hidden layer\n",
    "        out['out'] = nn.functional.softmax(self.output(x), dim=1)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "net = SkipGram(vocab_size=train_data.vocab_size, embedding_dim=1, h_dim=30, nwords=train_data.vocab_size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input(word_idx, vocab_size):\n",
    "    \"\"\"\n",
    "    Quick function for converting input to tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    x = torch.zeros(vocab_size).long()\n",
    "    x[word_idx] = 1.0\n",
    "    \n",
    "    return x\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.from_numpy(train_data.idx_pairs[:,0]), torch.from_numpy(train_data.idx_pairs[:,1]))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "valid = data_utils.TensorDataset(torch.from_numpy(valid_data.idx_pairs[:,0]), torch.from_numpy(valid_data.idx_pairs[:,1]))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.210247039794922 0.0\n"
     ]
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))\n",
    "output = net(data.long())\n",
    "loss = criterion(output['out'], target.long())\n",
    "acc = accuracy(output['out'], target.long())\n",
    "\n",
    "print(loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the network\n",
    "\n",
    "should implement batch sizes, as this is too slow :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 9.21, accs: 0.00\n",
      "Epoch: 0, valid loss: 9.21, accs: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dde40f98b815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "train_loss, train_accs = [],  []\n",
    "valid_loss, valid_accs = [],  []\n",
    "\n",
    "eval_every = 2\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):    \n",
    "    for data, target in train_loader:        \n",
    "        x = data.long()\n",
    "        y_true = target.long()\n",
    "        \n",
    "        output = net(x)\n",
    "        \n",
    "        loss = criterion(output['out'], y_true)\n",
    "        acc = accuracy(output['out'], y_true)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % eval_every == 0:\n",
    "        \n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        \n",
    "        for i, (val_data, val_target) in enumerate(valid_loader):\n",
    "            x = data.long()\n",
    "            y_true = target.long()\n",
    "            \n",
    "            output = net(x)\n",
    "            \n",
    "            loss = criterion(output['out'], y_true)\n",
    "            acc = accuracy(output['out'], y_true)\n",
    "            \n",
    "            val_losses += loss * valid_loader.batch_size\n",
    "            val_accs += acc * valid_loader.batch_size\n",
    "            val_lengths += valid_loader.batch_size\n",
    "            \n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        \n",
    "        print(\"Epoch: {}, train loss: {:.2f}, accs: {:.2f}\".format(epoch, np.mean(train_loss), np.mean(train_accs)))\n",
    "        print(\"Epoch: {}, valid loss: {:.2f}, accs: {:.2f}\".format(epoch, get_numpy(val_losses), get_numpy(val_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- should we include subsampling!? probably :)\n",
    "- padding?\n",
    "\n",
    "# Good urls\n",
    "https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "https://github.com/deborausujono/word2vecpy/blob/master/word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
