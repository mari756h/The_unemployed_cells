{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "V7SMXgK9mx4r",
    "outputId": "756c845e-115c-4b84-ec00-5dd0022b88e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
      "Requirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
      "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "!pip3 install torch -U\n",
    "!pip3 install torchvision -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyPvqAsRH9rC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmEddgoRH9rH"
   },
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRwHBWenH9rJ"
   },
   "outputs": [],
   "source": [
    "urls = ['https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_yI2jljH9rM"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_table(urls[0], header=None)\n",
    "df_valid = pd.read_table(urls[1], header=None)\n",
    "df_test = pd.read_table(urls[2], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGE3x-zYH9rR"
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        \n",
    "    \n",
    "    def readData(self, data, name, window_size=1, sep=None):\n",
    "        corpus = [x.split(sep) for x in data]\n",
    "        vocab = set([y for x in corpus for y in x])\n",
    "        \n",
    "        setattr(self, name + \"_corpus\", corpus)\n",
    "        setattr(self, name + \"_vocab\", vocab)\n",
    "        setattr(self, name + \"_vocab_size\", len(vocab))\n",
    "        \n",
    "        if name == \"train\":\n",
    "            self.createIndices(name, vocab=vocab, vocab_size=len(vocab))\n",
    "        \n",
    "        self.generate_pairs(name, window_size)\n",
    "    \n",
    "    def createIndices(self, name, vocab, vocab_size):\n",
    "        \n",
    "        word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "        idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n",
    "        \n",
    "        # error checks\n",
    "        assert len(word2idx) == vocab_size\n",
    "        assert len(idx2word) == vocab_size\n",
    "        \n",
    "        setattr(self, \"word2idx\", word2idx)\n",
    "        setattr(self, \"idx2word\", idx2word)\n",
    "        \n",
    "    def __str__(self, name):\n",
    "        return \"{0} sentences in {3} data, consisting of {1} unique words\".format(len(getattr(self, name + \"_vocab\")), \n",
    "                                                                                  getattr(self, name + \"_vocab_size\"), name)\n",
    "    \n",
    "    def generate_pairs(self, name, window_size):\n",
    "        corpus = getattr(self, name + \"_corpus\")\n",
    "        \n",
    "        idx_pairs = []\n",
    "\n",
    "        for sentence in corpus:\n",
    "            indices = [self.word2idx[word] for word in sentence]\n",
    "            # for each word, threated as center word\n",
    "            for center_word_pos in range(len(indices)):\n",
    "                # for each window position\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = indices[context_word_pos]\n",
    "                    idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "        setattr(self, name + \"_idx_pairs\",  np.array(idx_pairs))\n",
    "        \n",
    "#     def wordFrequencies(self, n=10, update=False):\n",
    "        \n",
    "#         if not hasattr(self, \"sorted_counts\") or update:\n",
    "#             flattened = [y for x in self.corpus for y in x]\n",
    "#             self.nwords = len(flattened)\n",
    "#             counts = Counter(flattened)\n",
    "\n",
    "#             self.sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            \n",
    "#             self.word2freq = {w: f/self.nwords for (w, f) in counts.items()}\n",
    "        \n",
    "#         print(\"Top {0} most common words\".format(n))\n",
    "#         for i in range(n):\n",
    "#             print(self.sorted_counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6cyWNfdH9rW"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader()\n",
    "dataloader.readData(data=df_train[0], name='train', window_size=1)\n",
    "\n",
    "dataloader.readData(data=df_valid[0], name='valid', window_size=1)\n",
    "\n",
    "dataloader.readData(data=df_test[0], name='test', window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERiN1FHmH9rc"
   },
   "source": [
    "# Skip-gram\n",
    "\n",
    "Objective of skip-gram model: figure out word representations that are useful for predicting the surrounding words in a sentence. \n",
    "\n",
    "Given a sequence of words for training, $w_1,w_2,w_3,...,w_T$, the objective is then to maximize the average log probability over the training context $c$:\n",
    "$$\\frac{1}{T}\\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq0} \\log p(w_{t+j}|w_t)$$\n",
    "\n",
    "$p(w_{t+j}|w_t)$ is defined by the softmax function $$p(w_{O}|w_I)=\\frac{\\exp\\big(v_{w_O}'^\\intercal v_{w_I}\\big)}{\\sum_{w=1}^W\\exp(v_w'^\\intercal v_{w_I})}$$\n",
    "\n",
    "Assumtions:\n",
    "* context windows are symmetrical with a value $x\\in \\R $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "ivZvKWh3H9rd",
    "outputId": "f330bd72-0297-4b76-f394-e4cdd103629c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (in_embeddings): Embedding(9999, 300, sparse=True)\n",
      "  (output): Linear(in_features=300, out_features=9999, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            vocab_size: number of vocab\n",
    "            embedding_dim: embedding dimensions\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.in_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, sparse=True)\n",
    "#         self.out_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, sparse=True)\n",
    "        \n",
    "        # hidden layer\n",
    "#         self.hidden = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        \n",
    "        # output layer        \n",
    "        self.output = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x, yt):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            x: input data\n",
    "            yt: true y\n",
    "        \n",
    "        Return:\n",
    "            out: predictions of words, dim corresponds to vocab size\n",
    "        \"\"\"\n",
    "        \n",
    "        # get embeddings for input x\n",
    "        x_vec = self.in_embeddings(x)\n",
    "        \n",
    "        # output\n",
    "        out = self.output(x_vec)\n",
    "        \n",
    "        # log probabilities\n",
    "        out_log = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        \n",
    "        return out_log\n",
    "        \n",
    "        \n",
    "net = SkipGram(vocab_size=dataloader.train_vocab_size, embedding_dim=300)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghUA9xXZH9rk"
   },
   "source": [
    "## Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4rMyP7SH9rl"
   },
   "outputs": [],
   "source": [
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G4IUl5JqH9rp",
    "outputId": "3c7ed39c-1cdc-41a2-cdc3-42ffa64077d5"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print(\"Cuda available\")\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbtKNyFyH9ru"
   },
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.from_numpy(dataloader.train_idx_pairs[:,0]), torch.from_numpy(dataloader.train_idx_pairs[:,1]))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "valid = data_utils.TensorDataset(torch.from_numpy(dataloader.valid_idx_pairs[:,0]), torch.from_numpy(dataloader.valid_idx_pairs[:,1]))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-uYHl0uH9rz"
   },
   "source": [
    "## Criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdwG_d9OH9rz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1)\n",
    "\n",
    "# add a schedular for dynamically updating the learning rate\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=2, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AguUltHpH9r4"
   },
   "source": [
    "## Test the network quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EIlHGuabH9r5",
    "outputId": "f72350b6-c88f-46d8-cdb8-c298dfb91393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.331087112426758 0.0\n"
     ]
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))\n",
    "\n",
    "if use_cuda:\n",
    "    data = data.cuda()\n",
    "    target = target.cuda()\n",
    "\n",
    "output = net(x = data.long(), yt=target.long())\n",
    "loss = criterion(output, target.long())\n",
    "acc = accuracy(output, target.long())\n",
    "\n",
    "print(loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MxbQRDNH9r_"
   },
   "source": [
    "## Training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3007
    },
    "colab_type": "code",
    "id": "e-YHDP6tH9sA",
    "outputId": "0598bfe6-e41e-490d-c57e-6bc9c2cea488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (in_embeddings): Embedding(9999, 300, sparse=True)\n",
      "  (output): Linear(in_features=300, out_features=9999, bias=True)\n",
      ")\n",
      "<NllLossBackward object at 0x00000235D185C080>\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 1\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 1, iteration 6001/26421, loss: 7.117, acc: 0.066\n",
      "Epoch 1, iteration 12001/26421, loss: 6.909, acc: 0.071\n",
      "Epoch 1, iteration 18001/26421, loss: 6.799, acc: 0.074\n",
      "Epoch 1, iteration 24001/26421, loss: 6.724, acc: 0.076\n",
      "\n",
      "Train loss: 6.700, accuracy: 0.077\n",
      "Validation loss: 6.543, accuracy: 0.078\n",
      "\n",
      "Epoch 2, iteration 6001/26421, loss: 6.142, acc: 0.085\n",
      "Epoch 2, iteration 12001/26421, loss: 6.164, acc: 0.087\n",
      "Epoch 2, iteration 18001/26421, loss: 6.174, acc: 0.088\n",
      "Epoch 2, iteration 24001/26421, loss: 6.178, acc: 0.088\n",
      "\n",
      "Train loss: 6.178, accuracy: 0.089\n",
      "Validation loss: 6.412, accuracy: 0.089\n",
      "\n",
      "Epoch 3, iteration 6001/26421, loss: 5.948, acc: 0.092\n",
      "Epoch 3, iteration 12001/26421, loss: 5.978, acc: 0.092\n",
      "Epoch 3, iteration 18001/26421, loss: 5.994, acc: 0.093\n",
      "Epoch 3, iteration 24001/26421, loss: 6.002, acc: 0.093\n",
      "\n",
      "Train loss: 6.003, accuracy: 0.093\n",
      "Validation loss: 6.373, accuracy: 0.094\n",
      "\n",
      "Epoch 4, iteration 6001/26421, loss: 5.832, acc: 0.095\n",
      "Epoch 4, iteration 12001/26421, loss: 5.864, acc: 0.096\n",
      "Epoch 4, iteration 18001/26421, loss: 5.880, acc: 0.096\n",
      "Epoch 4, iteration 24001/26421, loss: 5.893, acc: 0.097\n",
      "\n",
      "Train loss: 5.896, accuracy: 0.097\n",
      "Validation loss: 6.342, accuracy: 0.098\n",
      "\n",
      "Epoch 5, iteration 6001/26421, loss: 5.761, acc: 0.097\n",
      "Epoch 5, iteration 12001/26421, loss: 5.782, acc: 0.099\n",
      "Epoch 5, iteration 18001/26421, loss: 5.802, acc: 0.099\n",
      "Epoch 5, iteration 24001/26421, loss: 5.812, acc: 0.099\n",
      "\n",
      "Train loss: 5.815, accuracy: 0.099\n",
      "Validation loss: 6.329, accuracy: 0.096\n",
      "\n",
      "Epoch 6, iteration 6001/26421, loss: 5.688, acc: 0.101\n",
      "Epoch 6, iteration 12001/26421, loss: 5.717, acc: 0.101\n",
      "Epoch 6, iteration 18001/26421, loss: 5.733, acc: 0.102\n",
      "Epoch 6, iteration 24001/26421, loss: 5.747, acc: 0.102\n",
      "\n",
      "Train loss: 5.751, accuracy: 0.102\n",
      "Validation loss: 6.313, accuracy: 0.100\n",
      "\n",
      "Epoch 7, iteration 6001/26421, loss: 5.634, acc: 0.103\n",
      "Epoch 7, iteration 12001/26421, loss: 5.662, acc: 0.103\n",
      "Epoch 7, iteration 18001/26421, loss: 5.680, acc: 0.103\n",
      "Epoch 7, iteration 24001/26421, loss: 5.692, acc: 0.104\n",
      "\n",
      "Train loss: 5.697, accuracy: 0.104\n",
      "Validation loss: 6.287, accuracy: 0.098\n",
      "\n",
      "Epoch 8, iteration 6001/26421, loss: 5.583, acc: 0.105\n",
      "Epoch 8, iteration 12001/26421, loss: 5.613, acc: 0.105\n",
      "Epoch 8, iteration 18001/26421, loss: 5.631, acc: 0.105\n",
      "Epoch 8, iteration 24001/26421, loss: 5.645, acc: 0.105\n",
      "\n",
      "Train loss: 5.649, accuracy: 0.105\n",
      "Validation loss: 6.295, accuracy: 0.091\n",
      "\n",
      "Epoch 9, iteration 6001/26421, loss: 5.546, acc: 0.106\n",
      "Epoch 9, iteration 12001/26421, loss: 5.577, acc: 0.106\n",
      "Epoch 9, iteration 18001/26421, loss: 5.591, acc: 0.106\n",
      "Epoch 9, iteration 24001/26421, loss: 5.605, acc: 0.107\n",
      "\n",
      "Train loss: 5.609, accuracy: 0.107\n",
      "Validation loss: 6.283, accuracy: 0.106\n",
      "\n",
      "Epoch 10, iteration 6001/26421, loss: 5.510, acc: 0.106\n",
      "Epoch 10, iteration 12001/26421, loss: 5.536, acc: 0.107\n",
      "Epoch 10, iteration 18001/26421, loss: 5.553, acc: 0.107\n",
      "Epoch 10, iteration 24001/26421, loss: 5.567, acc: 0.108\n",
      "\n",
      "Train loss: 5.572, accuracy: 0.108\n",
      "Validation loss: 6.256, accuracy: 0.109\n",
      "\n",
      "Epoch 11, iteration 6001/26421, loss: 5.476, acc: 0.109\n",
      "Epoch 11, iteration 12001/26421, loss: 5.502, acc: 0.109\n",
      "Epoch 11, iteration 18001/26421, loss: 5.520, acc: 0.109\n",
      "Epoch 11, iteration 24001/26421, loss: 5.535, acc: 0.109\n",
      "\n",
      "Train loss: 5.539, accuracy: 0.109\n",
      "Validation loss: 6.263, accuracy: 0.109\n",
      "\n",
      "Epoch 12, iteration 6001/26421, loss: 5.445, acc: 0.110\n",
      "Epoch 12, iteration 12001/26421, loss: 5.474, acc: 0.110\n",
      "Epoch 12, iteration 18001/26421, loss: 5.491, acc: 0.110\n",
      "Epoch 12, iteration 24001/26421, loss: 5.505, acc: 0.110\n",
      "\n",
      "Train loss: 5.509, accuracy: 0.110\n",
      "Validation loss: 6.265, accuracy: 0.105\n",
      "\n",
      "Epoch 13, iteration 6001/26421, loss: 5.420, acc: 0.110\n",
      "Epoch 13, iteration 12001/26421, loss: 5.448, acc: 0.110\n",
      "Epoch 13, iteration 18001/26421, loss: 5.464, acc: 0.111\n",
      "Epoch 13, iteration 24001/26421, loss: 5.478, acc: 0.111\n",
      "\n",
      "Train loss: 5.482, accuracy: 0.111\n",
      "Validation loss: 6.254, accuracy: 0.110\n",
      "\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 14, iteration 6001/26421, loss: 5.156, acc: 0.133\n",
      "Epoch 14, iteration 12001/26421, loss: 5.132, acc: 0.135\n",
      "Epoch 14, iteration 18001/26421, loss: 5.127, acc: 0.136\n",
      "Epoch 14, iteration 24001/26421, loss: 5.123, acc: 0.136\n",
      "\n",
      "Train loss: 5.121, accuracy: 0.137\n",
      "Validation loss: 6.010, accuracy: 0.132\n",
      "\n",
      "Epoch 15, iteration 6001/26421, loss: 5.061, acc: 0.140\n",
      "Epoch 15, iteration 12001/26421, loss: 5.061, acc: 0.140\n",
      "Epoch 15, iteration 18001/26421, loss: 5.068, acc: 0.139\n",
      "Epoch 15, iteration 24001/26421, loss: 5.073, acc: 0.139\n",
      "\n",
      "Train loss: 5.075, accuracy: 0.139\n",
      "Validation loss: 6.003, accuracy: 0.133\n",
      "\n",
      "Epoch 16, iteration 6001/26421, loss: 5.047, acc: 0.139\n",
      "Epoch 16, iteration 12001/26421, loss: 5.049, acc: 0.139\n",
      "Epoch 16, iteration 18001/26421, loss: 5.055, acc: 0.140\n",
      "Epoch 16, iteration 24001/26421, loss: 5.062, acc: 0.139\n",
      "\n",
      "Train loss: 5.064, accuracy: 0.139\n",
      "Validation loss: 6.005, accuracy: 0.133\n",
      "\n",
      "Epoch 17, iteration 6001/26421, loss: 5.033, acc: 0.140\n",
      "Epoch 17, iteration 12001/26421, loss: 5.041, acc: 0.140\n",
      "Epoch 17, iteration 18001/26421, loss: 5.049, acc: 0.139\n",
      "Epoch 17, iteration 24001/26421, loss: 5.055, acc: 0.139\n",
      "\n",
      "Train loss: 5.057, accuracy: 0.139\n",
      "Validation loss: 6.010, accuracy: 0.131\n",
      "\n",
      "Epoch 18, iteration 6001/26421, loss: 5.026, acc: 0.140\n",
      "Epoch 18, iteration 12001/26421, loss: 5.036, acc: 0.140\n",
      "Epoch 18, iteration 18001/26421, loss: 5.044, acc: 0.139\n",
      "Epoch 18, iteration 24001/26421, loss: 5.050, acc: 0.139\n",
      "\n",
      "Train loss: 5.052, accuracy: 0.139\n",
      "Validation loss: 6.013, accuracy: 0.132\n",
      "\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 19, iteration 6001/26421, loss: 5.007, acc: 0.142\n",
      "Epoch 19, iteration 12001/26421, loss: 5.007, acc: 0.143\n",
      "Epoch 19, iteration 18001/26421, loss: 5.006, acc: 0.143\n",
      "Epoch 19, iteration 24001/26421, loss: 5.007, acc: 0.143\n",
      "\n",
      "Train loss: 5.007, accuracy: 0.143\n",
      "Validation loss: 6.001, accuracy: 0.136\n",
      "\n",
      "Epoch 20, iteration 6001/26421, loss: 5.006, acc: 0.144\n",
      "Epoch 20, iteration 12001/26421, loss: 5.003, acc: 0.144\n",
      "Epoch 20, iteration 18001/26421, loss: 5.004, acc: 0.144\n",
      "Epoch 20, iteration 24001/26421, loss: 5.003, acc: 0.144\n",
      "\n",
      "Train loss: 5.003, accuracy: 0.144\n",
      "Validation loss: 6.000, accuracy: 0.135\n",
      "\n",
      "Epoch 21, iteration 6001/26421, loss: 5.001, acc: 0.144\n",
      "Epoch 21, iteration 12001/26421, loss: 4.998, acc: 0.144\n",
      "Epoch 21, iteration 18001/26421, loss: 5.002, acc: 0.144\n",
      "Epoch 21, iteration 24001/26421, loss: 5.001, acc: 0.144\n",
      "\n",
      "Train loss: 5.002, accuracy: 0.144\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 22, iteration 6001/26421, loss: 4.991, acc: 0.144\n",
      "Epoch 22, iteration 12001/26421, loss: 4.992, acc: 0.145\n",
      "Epoch 22, iteration 18001/26421, loss: 4.994, acc: 0.145\n",
      "Epoch 22, iteration 24001/26421, loss: 4.996, acc: 0.145\n",
      "\n",
      "Train loss: 4.996, accuracy: 0.144\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch 23, iteration 6001/26421, loss: 4.990, acc: 0.145\n",
      "Epoch 23, iteration 12001/26421, loss: 4.992, acc: 0.145\n",
      "Epoch 23, iteration 18001/26421, loss: 4.993, acc: 0.145\n",
      "Epoch 23, iteration 24001/26421, loss: 4.996, acc: 0.144\n",
      "\n",
      "Train loss: 4.996, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch 24, iteration 6001/26421, loss: 4.995, acc: 0.145\n",
      "Epoch 24, iteration 12001/26421, loss: 4.995, acc: 0.144\n",
      "Epoch 24, iteration 18001/26421, loss: 4.996, acc: 0.145\n",
      "Epoch 24, iteration 24001/26421, loss: 4.996, acc: 0.144\n",
      "\n",
      "Train loss: 4.996, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 25, iteration 6001/26421, loss: 4.997, acc: 0.145\n",
      "Epoch 25, iteration 12001/26421, loss: 4.994, acc: 0.145\n",
      "Epoch 25, iteration 18001/26421, loss: 4.995, acc: 0.145\n",
      "Epoch 25, iteration 24001/26421, loss: 4.995, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 5.999, accuracy: 0.136\n",
      "\n",
      "Epoch 26, iteration 6001/26421, loss: 4.998, acc: 0.144\n",
      "Epoch 26, iteration 12001/26421, loss: 4.999, acc: 0.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, iteration 18001/26421, loss: 4.998, acc: 0.144\n",
      "Epoch 26, iteration 24001/26421, loss: 4.995, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 5.999, accuracy: 0.136\n",
      "\n",
      "Epoch 27, iteration 6001/26421, loss: 4.991, acc: 0.145\n",
      "Epoch 27, iteration 12001/26421, loss: 4.993, acc: 0.145\n",
      "Epoch 27, iteration 18001/26421, loss: 4.994, acc: 0.145\n",
      "Epoch 27, iteration 24001/26421, loss: 4.996, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 28, iteration 6001/26421, loss: 4.998, acc: 0.144\n",
      "Epoch 28, iteration 12001/26421, loss: 4.998, acc: 0.144\n",
      "Epoch 28, iteration 18001/26421, loss: 4.996, acc: 0.144\n",
      "Epoch 28, iteration 24001/26421, loss: 4.995, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch 29, iteration 6001/26421, loss: 4.997, acc: 0.144\n",
      "Epoch 29, iteration 12001/26421, loss: 4.994, acc: 0.145\n",
      "Epoch 29, iteration 18001/26421, loss: 4.996, acc: 0.145\n",
      "Epoch 29, iteration 24001/26421, loss: 4.995, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch 30, iteration 6001/26421, loss: 4.999, acc: 0.145\n",
      "Epoch 30, iteration 12001/26421, loss: 4.994, acc: 0.145\n",
      "Epoch 30, iteration 18001/26421, loss: 4.996, acc: 0.145\n",
      "Epoch 30, iteration 24001/26421, loss: 4.995, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 31, iteration 6001/26421, loss: 4.998, acc: 0.144\n",
      "Epoch 31, iteration 12001/26421, loss: 4.995, acc: 0.145\n",
      "Epoch 31, iteration 18001/26421, loss: 4.996, acc: 0.144\n",
      "Epoch 31, iteration 24001/26421, loss: 4.994, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch 32, iteration 6001/26421, loss: 4.992, acc: 0.146\n",
      "Epoch 32, iteration 12001/26421, loss: 4.994, acc: 0.145\n",
      "Epoch 32, iteration 18001/26421, loss: 4.995, acc: 0.145\n",
      "Epoch 32, iteration 24001/26421, loss: 4.995, acc: 0.145\n",
      "\n",
      "Train loss: 4.995, accuracy: 0.145\n",
      "Validation loss: 6.000, accuracy: 0.136\n",
      "\n",
      "Epoch 33, iteration 6001/26421, loss: 4.994, acc: 0.145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7c55b7a1de75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(net)\n",
    "print(loss.grad_fn)\n",
    "print(optimizer)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "N = len(train_loader)\n",
    "log_every = 6000\n",
    "\n",
    "epoch_train_loss, epoch_train_acc, epoch_valid_loss, epoch_valid_acc = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc, train_lengths = 0.0, 0.0, 0.0\n",
    "    \n",
    "    net.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # check if cuda is available\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        output = net(inputs.long(), labels.long())\n",
    "        loss = criterion(output, labels.long())\n",
    "        acc = accuracy(output, labels.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * train_loader.batch_size\n",
    "        train_acc += acc.item() * train_loader.batch_size\n",
    "        train_lengths += train_loader.batch_size\n",
    "        \n",
    "        if i % log_every == 0 and i > 0:\n",
    "            print(\"Epoch {}, iteration {}/{}, loss: {:.3f}, acc: {:.3f}\".format(epoch+1, i+1, N, train_loss / train_lengths, train_acc / train_lengths))\n",
    "            \n",
    "    \n",
    "    net.eval()\n",
    "    val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "    for j, (inputs, labels) in enumerate(valid_loader):\n",
    "        # check if cuda is available\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        # forward + backward\n",
    "        output= net(inputs.long(), labels.long())\n",
    "        loss = criterion(output, labels.long())\n",
    "        \n",
    "        val_losses += loss.item() * valid_loader.batch_size\n",
    "        val_accs += accuracy(output, labels.long()).item() * valid_loader.batch_size\n",
    "        val_lengths += valid_loader.batch_size\n",
    "        \n",
    "    val_losses /= val_lengths\n",
    "    val_accs /= val_lengths\n",
    "    \n",
    "    train_loss /= train_lengths\n",
    "    train_acc /= train_lengths\n",
    "    \n",
    "    epoch_train_acc.append(train_acc)\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_valid_acc.append(val_accs)\n",
    "    epoch_valid_loss.append(val_losses)\n",
    "    \n",
    "    print(\"\\nTrain loss: {:.3f}, accuracy: {:.3f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation loss: {:.3f}, accuracy: {:.3f}\\n\".format(val_losses, val_accs))\n",
    "    \n",
    "    # adjust learning rate if needed\n",
    "    scheduler.step(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWcn1TAzH9sH"
   },
   "source": [
    "# Notes\n",
    "- should we include subsampling!? probably :)\n",
    "- padding?\n",
    "\n",
    "# Good urls\n",
    "https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "https://github.com/deborausujono/word2vecpy/blob/master/word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjOugHIAH9sJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "skip-grams.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
