{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table(urls[0], header=None)\n",
    "df_valid = pd.read_table(urls[1], header=None)\n",
    "df_test = pd.read_table(urls[2], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        \n",
    "    \n",
    "    def readData(self, data, name, window_size=1, sep=None):\n",
    "        corpus = [x.split(sep) for x in data]\n",
    "        vocab = set([y for x in corpus for y in x])\n",
    "        \n",
    "        setattr(self, name + \"_corpus\", corpus)\n",
    "        setattr(self, name + \"_vocab\", vocab)\n",
    "        \n",
    "        if name == \"train\":\n",
    "            self.createIndices(name, vocab=vocab, vocab_size=len(vocab))\n",
    "        \n",
    "        self.generate_pairs(name, window_size)\n",
    "    \n",
    "    def createIndices(self, name, vocab, vocab_size):\n",
    "        \n",
    "        word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "        idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n",
    "        \n",
    "        # error checks\n",
    "        assert len(word2idx) == vocab_size\n",
    "        assert len(idx2word) == vocab_size\n",
    "        \n",
    "        setattr(self, \"word2idx\", word2idx)\n",
    "        setattr(self, \"idx2word\", idx2word)\n",
    "        setattr(self, \"vocab_size\", vocab_size)\n",
    "        \n",
    "    def __str__(self, name):\n",
    "        return \"{0} sentences in {3} data, consisting of {1} unique words\".format(len(getattr(self, name + \"_vocab\")), \n",
    "                                                                                  getattr(self, name + \"_vocab_size\"), name)\n",
    "    \n",
    "    def generate_pairs(self, name, window_size):\n",
    "        corpus = getattr(self, name + \"_corpus\")\n",
    "        \n",
    "        idx_pairs = []\n",
    "\n",
    "        for sentence in corpus:\n",
    "            indices = [self.word2idx[word] for word in sentence]\n",
    "            # for each word, threated as center word\n",
    "            for center_word_pos in range(len(indices)):\n",
    "                # for each window position\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = indices[context_word_pos]\n",
    "                    idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "        setattr(self, name + \"_idx_pairs\",  np.array(idx_pairs))\n",
    "        \n",
    "#     def wordFrequencies(self, n=10, update=False):\n",
    "        \n",
    "#         if not hasattr(self, \"sorted_counts\") or update:\n",
    "#             flattened = [y for x in self.corpus for y in x]\n",
    "#             self.nwords = len(flattened)\n",
    "#             counts = Counter(flattened)\n",
    "\n",
    "#             self.sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            \n",
    "#             self.word2freq = {w: f/self.nwords for (w, f) in counts.items()}\n",
    "        \n",
    "#         print(\"Top {0} most common words\".format(n))\n",
    "#         for i in range(n):\n",
    "#             print(self.sorted_counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader()\n",
    "dataloader.readData(data=df_train[0], name='train')\n",
    "\n",
    "dataloader.readData(data=df_valid[0], name='valid')\n",
    "\n",
    "dataloader.readData(data=df_test[0], name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram\n",
    "\n",
    "Objective of skip-gram model: figure out word representations that are useful for predicting the surrounding words in a sentence. \n",
    "\n",
    "Given a sequence of words for training, $w_1,w_2,w_3,...,w_T$, the objective is then to maximize the average log probability over the training context $c$:\n",
    "$$\\frac{1}{T}\\sum_{t=1}^T\\sum_{-c\\leq j \\leq c, j\\neq0} \\log p(w_{t+j}|w_t)$$\n",
    "\n",
    "$p(w_{t+j}|w_t)$ is defined by the softmax function $$p(w_{O}|w_I)=\\frac{\\exp\\big(v_{w_O}'^\\intercal v_{w_I}\\big)}{\\sum_{w=1}^W\\exp(v_w'^\\intercal v_{w_I})}$$\n",
    "\n",
    "Assumtions:\n",
    "* context windows are symmetrical with a value $x\\in \\R $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (in_embeddings): Embedding(9999, 100, sparse=True)\n",
      "  (output): Linear(in_features=100, out_features=9999, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            vocab_size: number of vocab\n",
    "            embedding_dim: embedding dimensions\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.in_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, sparse=True)\n",
    "#         self.out_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, sparse=True)\n",
    "        \n",
    "        # hidden layer\n",
    "#         self.hidden = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        \n",
    "        # output layer        \n",
    "        self.output = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x, yt):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            x: input data\n",
    "            yt: true y\n",
    "        \n",
    "        Return:\n",
    "            out: predictions of words, dim corresponds to vocab size\n",
    "        \"\"\"\n",
    "        \n",
    "        # get embeddings for input x\n",
    "        x_vec = self.in_embeddings(x)\n",
    "        \n",
    "        # output\n",
    "        out = self.output(x_vec)\n",
    "        \n",
    "        # log probabilities\n",
    "        out_log = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out_log\n",
    "\n",
    "net = SkipGram(vocab_size=dataloader.vocab_size, embedding_dim=100)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ys, ts):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        ys: predicted values\n",
    "        ts: true values\n",
    "    \"\"\"\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print(\"Cuda available\")\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.from_numpy(dataloader.train_idx_pairs[:,0]), torch.from_numpy(dataloader.train_idx_pairs[:,1]))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "valid = data_utils.TensorDataset(torch.from_numpy(dataloader.valid_idx_pairs[:,0]), torch.from_numpy(dataloader.valid_idx_pairs[:,1]))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.210245132446289 0.0\n"
     ]
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))\n",
    "\n",
    "if use_cuda:\n",
    "    data = data.cuda()\n",
    "    target = target.cuda()\n",
    "\n",
    "output = net(x = data.long(), yt=target.long())\n",
    "loss = criterion(output, target.long())\n",
    "acc = accuracy(output, target.long())\n",
    "\n",
    "print(loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the network\n",
    "\n",
    "should implement batch sizes, as this is too slow :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iteration 1/13211, loss: 0.005, acc: 0.000\n",
      "Epoch 1, iteration 2001/13211, loss: 9.210, acc: 0.000\n",
      "Epoch 1, iteration 4001/13211, loss: 9.210, acc: 0.000\n",
      "Epoch 1, iteration 6001/13211, loss: 9.210, acc: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-2047f363dc15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "log_every = 2000\n",
    "N = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    \n",
    "    net.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # check if cuda is available\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        output = net(inputs.long(), labels.long())\n",
    "        loss = criterion(output, labels.long())\n",
    "        acc = accuracy(output, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc.item()\n",
    "        \n",
    "        if i % log_every == 0:\n",
    "            print(\"Epoch {}, iteration {}/{}, loss: {:.3f}, acc: {:.3f}\".format(epoch+1, i+1, N, \n",
    "                                                                                running_loss / log_every, \n",
    "                                                                                running_acc / log_every))\n",
    "            \n",
    "            running_loss, running_acc = 0.0, 0.0\n",
    "    \n",
    "    net.eval()\n",
    "    val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "    for j, (inputs, labels) in enumerate(valid_loader):\n",
    "        # check if cuda is available\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        # forward + backward\n",
    "        output = net(inputs.long(), labels.long())\n",
    "        loss = criterion(output, labels.long())\n",
    "        \n",
    "        val_losses += loss.item() * valid_loader.batch_size\n",
    "        val_accs += accuracy(output, labels.long()).item() * valid_loader.batch_size\n",
    "        val_lengths += valid_loader.batch_size\n",
    "        \n",
    "    val_losses /= val_lengths\n",
    "    val_accs /= val_lengths\n",
    "    \n",
    "    print(\"\\nValidation loss: {:.3f}, accuracy: {:.3f}\\n\".format(val_losses, val_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- should we include subsampling!? probably :)\n",
    "- padding?\n",
    "\n",
    "# Good urls\n",
    "https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "https://github.com/deborausujono/word2vecpy/blob/master/word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
