{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10da99cb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import torch\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test. Data was downloaded from [train](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt), [validation](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt) and [test](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_counts(path, verbose=False): \n",
    "    # Open training file \n",
    "    corpus = []\n",
    "    with open(path, 'r') as infile: \n",
    "        for line in infile: \n",
    "            line = line[:-1].split()\n",
    "            corpus.append(line)\n",
    "            \n",
    "\n",
    "    # Count occurrences\n",
    "    unique, counts = np.unique(np.array([item for sublist in corpus for item in sublist]), return_counts=True)\n",
    "    corpus_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    if verbose: \n",
    "        for v, k in sorted(zip(counts, unique), reverse=True): \n",
    "            print('Key is \"{0}\" with count {1}'.format(k, v))\n",
    "\n",
    "    return corpus, corpus_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'this', 'is', 'awesome', 'to', 'be', 'part', 'of']\n",
      "['today', 'is', 'a', 'nice', 'day']\n",
      "['deep', 'learning', 'is', 'cool', 'to', 'do']\n",
      "['have', 'you', 'seen', 'the', 'board', 'today']\n",
      "['are', 'you', 'feeling', 'okay', 'with', 'this']\n",
      "['i', 'like', 'trains', 'very', 'much']\n"
     ]
    }
   ],
   "source": [
    "### Train_iterator\n",
    "def train_loader(path, context_size): \n",
    "    # Build vocabulary\n",
    "    train, train_counts = corpus_counts(path, verbose=False)\n",
    "    vocab_size = len(train_counts)\n",
    "    word_to_idx = {word: i for i, word in enumerate(train_counts.keys())}\n",
    "    \n",
    "    # Make contexts\n",
    "    word_data, word_targets = [], []\n",
    "    for line in train: \n",
    "        print(line)\n",
    "        sample, target = [], []\n",
    "        for i in range(context_size, len(line) - context_size):\n",
    "            context, c = [], -context_size\n",
    "            while c <= context_size:\n",
    "                if c != 0: \n",
    "                    context.append(line[i+c])\n",
    "                c += 1\n",
    "            target.append(line[i])\n",
    "            sample.append(context)\n",
    "            \n",
    "            # Make context vector\n",
    "            vec_context = make_context_vector(context=context, word_to_ix=word_to_idx)\n",
    "            vec_target = make_context_vector(context=target, word_to_ix=word_to_idx)\n",
    "            \n",
    "        word_data.append(sample)\n",
    "        word_targets.append(target)\n",
    "    \n",
    "    return word_data, word_targets\n",
    "\n",
    "# Call function\n",
    "word_data, word_targets = train_loader(path='data/temp.txt', context_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'world', 'is', 'awesome'], ['world', 'this', 'awesome', 'to'], ['this', 'is', 'to', 'be'], ['is', 'awesome', 'be', 'part'], ['awesome', 'to', 'part', 'of']] ['this', 'is', 'awesome', 'to', 'be'] \n",
      "\n",
      "[['today', 'is', 'nice', 'day']] ['a'] \n",
      "\n",
      "[['deep', 'learning', 'cool', 'to'], ['learning', 'is', 'to', 'do']] ['is', 'cool'] \n",
      "\n",
      "[['have', 'you', 'the', 'board'], ['you', 'seen', 'board', 'today']] ['seen', 'the'] \n",
      "\n",
      "[['are', 'you', 'okay', 'with'], ['you', 'feeling', 'with', 'this']] ['feeling', 'okay'] \n",
      "\n",
      "[['i', 'like', 'very', 'much']] ['trains'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample, target in zip(word_data, word_targets): \n",
    "    print(sample, target, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes \n",
    "Consider including padding in loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
