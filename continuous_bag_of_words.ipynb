{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters \n",
    "pad = False  # Padding\n",
    "ws = 2       # Window_size\n",
    "bs = 1000    # Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test. Data was downloaded from [train](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt), [validation](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt) and [test](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader class\n",
    "class DataLoader: \n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "    \n",
    "    # Load words\n",
    "    def load_corpus(self, path): \n",
    "        with open(path, 'r') as infile: \n",
    "            for line in infile: \n",
    "                line = line[:-1].split()\n",
    "                self.corpus.append(line)\n",
    "    \n",
    "    # Make dict\n",
    "    def count_corpus(self, padding=True, verbose=False): \n",
    "        # Count occurrences\n",
    "        unique, counts = np.unique(np.array([item for sublist in self.corpus for item in sublist]), return_counts=True)\n",
    "        self.corpus_counts = dict(zip(unique, counts))\n",
    "\n",
    "        if verbose: \n",
    "            for v, k in sorted(zip(counts, unique), reverse=True): \n",
    "                print('Key is \"{0}\" with count {1}'.format(k, v))\n",
    "\n",
    "        # Build vocabulary\n",
    "        if padding: \n",
    "            indices = list(range(1,len(unique+1)))\n",
    "            self.word_to_idx = dict(zip(sorted(unique, indices)))\n",
    "            #self.word_to_idx = {word: i+1 for i, word in enumerate(self.corpus_counts.keys())}\n",
    "            self.word_to_idx['padding'] = 0\n",
    "        else: \n",
    "            indices = list(range(len(unique)))\n",
    "            self.word_to_idx = dict(zip(sorted(unique), indices))\n",
    "            #self.word_to_idx = {word: i for i, word in enumerate(self.corpus_counts.keys())}\n",
    "        \n",
    "        # Make reverse dict\n",
    "        self.idx_to_word = {idx: w for idx, w in self.word_to_idx.items()}\n",
    "            \n",
    "    # Function to make context pairs\n",
    "    def make_context_pairs(self, window_size=2, padding=True): \n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Run through each sample\n",
    "        self.word_data = []\n",
    "        for line in self.corpus: \n",
    "            if padding: \n",
    "                # Add padding corresponding to the size of the window on either side\n",
    "                padding = ['padding']*window_size\n",
    "                line = padding+line+padding\n",
    "\n",
    "            # Make contexts\n",
    "            for i in range(window_size, len(line) - window_size):\n",
    "                context, c = [], -window_size\n",
    "                while c <= window_size:\n",
    "                    if c != 0: \n",
    "                        context.append(line[i+c])\n",
    "                    c += 1\n",
    "                self.word_data.append((context, line[i]))\n",
    "\n",
    "    # Convert word_data to numpy array tuples\n",
    "    def words_to_index(self, word2idx):\n",
    "        if hasattr(self, 'window_size'): \n",
    "            # Pre-allocate\n",
    "            data = np.empty((len(self.word_data), self.window_size*2), dtype=int)\n",
    "            labels = np.empty((len(self.word_data)), dtype=int)\n",
    "            print(data.shape)\n",
    "            \n",
    "            # Run through context pairs and fill arrays\n",
    "            i = 0\n",
    "            for d, l in self.word_data: \n",
    "                data[i, :] = np.array([word2idx[w] for w in d])\n",
    "                labels[i,] = word2idx[l]\n",
    " \n",
    "                i += 1\n",
    "                \n",
    "            # Save as tuple\n",
    "            self.context_array = (data, labels)\n",
    "        else: \n",
    "            print('# Make context pairs, first!')\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "train_data = DataLoader()\n",
    "train_data.load_corpus(path='data/ptb.train.txt')\n",
    "train_data.count_corpus(padding=pad)\n",
    "\n",
    "# Make context pairs for training data\n",
    "train_data.make_context_pairs(window_size=ws, padding=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720533\n",
      "['aer', 'banknote', 'calloway', 'centrust'] berlitz\n",
      "['banknote', 'berlitz', 'centrust', 'cluett'] calloway\n",
      "['berlitz', 'calloway', 'cluett', 'fromstein'] centrust\n",
      "['calloway', 'centrust', 'fromstein', 'gitano'] cluett\n",
      "['centrust', 'cluett', 'gitano', 'guterman'] fromstein\n",
      "['cluett', 'fromstein', 'guterman', 'hydro-quebec'] gitano\n",
      "['fromstein', 'gitano', 'hydro-quebec', 'ipo'] guterman\n",
      "['gitano', 'guterman', 'ipo', 'kia'] hydro-quebec\n",
      "['guterman', 'hydro-quebec', 'kia', 'memotec'] ipo\n",
      "['hydro-quebec', 'ipo', 'memotec', 'mlx'] kia\n"
     ]
    }
   ],
   "source": [
    "# Check word contexts\n",
    "word_sum = len(train_data.word_data)\n",
    "print(word_sum)\n",
    "for context, word in train_data.word_data[:10]: \n",
    "    print(context, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720533, 4)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "train_data.words_to_index(word2idx=train_data.word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57004, 4)\n"
     ]
    }
   ],
   "source": [
    "# Get validation data\n",
    "valid_data = DataLoader()\n",
    "valid_data.load_corpus(path='data/ptb.valid.txt')\n",
    "\n",
    "# Make context pairs for validation data\n",
    "valid_data.make_context_pairs(window_size=ws, padding=pad)\n",
    "\n",
    "# Convert to numpy\n",
    "valid_data.words_to_index(word2idx=train_data.word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data has been loaded it is good to check what is looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:\t (720533, 4)\n",
      "Number of validation samples:\t (57004, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples:\\t', train_data.context_array[0].shape)\n",
    "print('Number of validation samples:\\t', valid_data.context_array[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CBOW class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cbow(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, window_size, embedding_dim=2, n_hid=128, padding=True):\n",
    "        super(cbow, self).__init__()\n",
    "        # num_embeddings is the number of words in your train, val and test set\n",
    "        # embedding_dim is the dimension of the word vectors you are using\n",
    "        if padding: \n",
    "            self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, \n",
    "                                          padding_idx=0)\n",
    "        else: \n",
    "            self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, \n",
    "                                          padding_idx=None)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features=window_size * embedding_dim, out_features=n_hid, bias=True)\n",
    "        self.linear2 = nn.Linear(in_features=n_hid, out_features=vocab_size, bias=False)\n",
    "        self.window = window_size\n",
    "        self.embed_dim = embedding_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        #print(embeds.shape, embeds.view((-1, self.window*self.embed_dim)).shape)\n",
    "        out = F.relu(self.linear1(embeds.view((-1, self.window*self.embed_dim))))\n",
    "        out = self.linear2(out)\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate performance\n",
    "def accuracy(y_true, y_pred):\n",
    "    # Make y_pred for the word with max probability\n",
    "    values, indices = torch.max(input=y_pred, dim=1)\n",
    "    \n",
    "    # Check if indices match\n",
    "    check = torch.eq(indices, y_true)\n",
    "    \n",
    "    # Estimate accuracy\n",
    "    acc = check.sum()/len(check)\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch batch_loader\n",
    "train = data_utils.TensorDataset(torch.from_numpy(train_data.context_array[0]), torch.from_numpy(train_data.context_array[1]))\n",
    "load_train = data_utils.DataLoader(train, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid = data_utils.TensorDataset(torch.from_numpy(valid_data.context_array[0]), torch.from_numpy(valid_data.context_array[1]))\n",
    "load_valid = data_utils.DataLoader(valid, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss, model and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = cbow(vocab_size=len(train_data.word_to_idx), window_size=ws*2, embedding_dim=100, n_hid=100, padding=pad)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cbow(\n",
       "  (embeddings): Embedding(9999, 100)\n",
       "  (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=9999, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at loaded model\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.210275650024414 \tAccuracy: 0\n"
     ]
    }
   ],
   "source": [
    "# Test the network \n",
    "data, target = next(iter(load_train))\n",
    "output = net(data.long())\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Estimate accuracy\n",
    "acc = accuracy(y_true=target, y_pred=output)\n",
    "\n",
    "print('Loss:', loss.item(), '\\tAccuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch slice\n",
    "def get_batch(batch_size, i):\n",
    "    start_idx = i*batch_size\n",
    "    end_idx = (i+1)*batch_size\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def batch_to_context(batch, word_to_idx): \n",
    "    x_batch, y_batch =  [], []\n",
    "    \n",
    "    # Run through samples\n",
    "    for context, target in batch: \n",
    "        x = [word_to_idx[w] for w in context]\n",
    "        y = word_to_idx[target]\n",
    "        \n",
    "        # Append idx words to batch\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "    \n",
    "    # Make tensors\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long)\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch: 1/721, Loss: 9.210273742675781\n",
      "Epoch 1/10, Batch: 51/721, Loss: 9.119373321533203\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "max_epochs = 10\n",
    "\n",
    "# Run through epochs\n",
    "for epoch in range(max_epochs):\n",
    "    #print('# Epoch {0}/{1}'.format(epoch+1, max_epochs))\n",
    "    \n",
    "    ### Train ###\n",
    "    current_loss = 0\n",
    "    net.train()\n",
    "    for i, (inputs, labels) in enumerate(load_train):\n",
    "\n",
    "        # Zero gradient\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting probabilities over next words\n",
    "        probs = net(inputs)\n",
    "\n",
    "        # Step 4. Compute your loss function.\n",
    "        loss = criterion(probs, labels)\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        current_loss += loss.item()  \n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('Epoch {0}/{1}, Batch: {2}/{3}, Loss: {4}'.format(epoch+1, max_epochs, i+1, \n",
    "                                                                    len(load_train), current_loss/(i+1)))\n",
    "        \n",
    "    losses.append(current_loss)\n",
    "\n",
    "    \n",
    "    ### Evaluate training ###\n",
    "    net.eval()\n",
    "    train_preds, train_targs = [], []\n",
    "    for i, (inputs, labels) in enumerate(load_train):\n",
    "        # Get predictions\n",
    "        output = net(inputs)\n",
    "        preds = torch.max(inputs=output, dim=1)[1]\n",
    "        \n",
    "        train_targs += list(y_batch)\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation ###\n",
    "    val_preds, val_targs = [], []\n",
    "    for i, (inputs, labels) in enumerate(load_train):\n",
    "        output = net(inputs)\n",
    "        preds = torch.max(inputs=output, dim=1)[1]\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        val_targs += list(y_batch)\n",
    "\n",
    "    train_acc_cur = accuracy(y_true=train_targs, y_pred=train_preds)\n",
    "    valid_acc_cur = accuracy(y_true=val_targs, y_pred=val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\\n\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)  # The loss does not decrease that much..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation performances\n",
    "epoch = np.arange(max_epochs)\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Acc', 'Val Acc'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate test set\n",
    "#x_batch = Variable(torch.from_numpy(x_test))\n",
    "#output = net(x_batch)\n",
    "#preds = torch.max(output, 1)[1]\n",
    "#print(\"\\nTest set Acc:  %f\" % (accuracy_score(list(targets_test), list(preds.data.numpy()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
