{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104466bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test. Data was downloaded from [train](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt), [validation](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt) and [test](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(path): \n",
    "    corpus = []\n",
    "    with open(path, 'r') as infile: \n",
    "        for line in infile: \n",
    "            line = line[:-1].split()\n",
    "            corpus.append(line)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make corpus\n",
    "def corpus_counts(path, verbose=False): \n",
    "    # Open training file \n",
    "    corpus = load_corpus(path=path)\n",
    "\n",
    "    # Count occurrences\n",
    "    unique, counts = np.unique(np.array([item for sublist in corpus for item in sublist]), return_counts=True)\n",
    "    corpus_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    if verbose: \n",
    "        for v, k in sorted(zip(counts, unique), reverse=True): \n",
    "            print('Key is \"{0}\" with count {1}'.format(k, v))\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab_size = len(corpus_counts)\n",
    "    word_to_idx = {word: i+1 for i, word in enumerate(corpus_counts.keys())}\n",
    "    word_to_idx['padding'] = 0\n",
    "    \n",
    "    return corpus, corpus_counts, word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make context pairs\n",
    "def make_context_pairs(data, word_to_idx, window_size=2): \n",
    "    # Run through each sample\n",
    "    word_data = []\n",
    "    for line in data: \n",
    "        # Add padding corresponding to the size of the window on either side\n",
    "        padding = ['padding']*window_size\n",
    "        line = padding+line+padding\n",
    "        \n",
    "        # Make contexts\n",
    "        for i in range(window_size, len(line) - window_size):\n",
    "            context, c = [], -window_size\n",
    "            while c <= window_size:\n",
    "                if c != 0: \n",
    "                    context.append(line[i+c])\n",
    "                c += 1\n",
    "            word_data.append((context, line[i]))\n",
    "    \n",
    "    return word_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make corpus\n",
    "train, train_counts, word_to_idx = corpus_counts(path='data/ptb.train.txt', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set window size \n",
    "ws = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "#words, word_to_idx = data_loader(path='data/ptb.train.txt', window_size=2)\n",
    "train_words = make_context_pairs(data=train, word_to_idx=word_to_idx, window_size=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887521\n",
      "['padding', 'banknote'] aer\n",
      "['aer', 'berlitz'] banknote\n",
      "['banknote', 'calloway'] berlitz\n",
      "['berlitz', 'centrust'] calloway\n",
      "['calloway', 'cluett'] centrust\n",
      "['centrust', 'fromstein'] cluett\n",
      "['cluett', 'gitano'] fromstein\n",
      "['fromstein', 'guterman'] gitano\n",
      "['gitano', 'hydro-quebec'] guterman\n",
      "['guterman', 'ipo'] hydro-quebec\n"
     ]
    }
   ],
   "source": [
    "# Check word contexts\n",
    "word_sum = len(train_words)\n",
    "print(word_sum)\n",
    "for context, word in train_words[:10]: \n",
    "    #s, t = context_pair\n",
    "    #print(s, t)\n",
    "    print(context, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "valid = load_corpus(path='data/ptb.valid.txt')\n",
    "valid_words = make_context_pairs(data=valid, word_to_idx=word_to_idx, window_size=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data has been loaded it is good to check what is looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:\t 887521\n",
      "Number of validation samples:\t 70390\n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples:\\t', len(train_words))\n",
    "print('Number of validation samples:\\t', len(valid_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CBOW class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cbow(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, window_size):\n",
    "        super(cbow, self).__init__()\n",
    "        # num_embeddings is the number of words in your train, val and test set\n",
    "        # embedding_dim is the dimension of the word vectors you are using\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, \n",
    "                                      padding_idx=0)\n",
    "        self.linear1 = nn.Linear(in_features=window_size * embedding_dim, out_features=128, bias=True)\n",
    "        self.linear2 = nn.Linear(in_features=128, out_features=vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss, model and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = cbow(vocab_size=len(train_counts), embedding_dim=4, window_size=ws*2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cbow(\n",
       "  (embeddings): Embedding(9999, 4, padding_idx=0)\n",
       "  (linear1): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=9999, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at loaded model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Epoch 1/10\n",
      "\n",
      "# Epoch 2/10\n",
      "\n",
      "# Epoch 3/10\n",
      "\n",
      "# Epoch 4/10\n",
      "\n",
      "# Epoch 5/10\n",
      "\n",
      "# Epoch 6/10\n",
      "\n",
      "# Epoch 7/10\n",
      "\n",
      "# Epoch 8/10\n",
      "\n",
      "# Epoch 9/10\n",
      "\n",
      "# Epoch 10/10\n",
      "[921.024097442627, 921.024097442627, 921.024097442627, 921.024097442627, 921.024097442627, 921.024097442627, 921.024097442627, 921.0240964889526, 921.0240964889526, 921.0240964889526]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "max_epochs = 10\n",
    "max_samples = 100\n",
    "train_batch = train_words[:max_samples]\n",
    "for epoch in range(max_epochs):\n",
    "    print('\\n# Epoch {0}/{1}'.format(epoch+1, max_epochs))\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    model.train()\n",
    "    for context, target in train_batch:\n",
    "        #print('\\tSample {0}/{1}'.format(i, max_samples))\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        vec_context = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n",
    "        #vec_context = make_context_vector(context=context, word_to_ix=word_to_idx)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting probabilities over next words\n",
    "        probs = model(vec_context)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(probs, torch.tensor([word_to_idx[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        i += 1\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
