{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10da99cb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import torch\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The Penn Treebank datafiles are given in the urls below, where we have three different datasets: train, validation and test. Data was downloaded from [train](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt), [validation](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt) and [test](https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_counts(path, verbose=False): \n",
    "    # Open training file \n",
    "    corpus = []\n",
    "    with open(path, 'r') as infile: \n",
    "        for line in infile: \n",
    "            line = line[:-1].split()\n",
    "            corpus.append(line)\n",
    "            \n",
    "\n",
    "    # Count occurrences\n",
    "    unique, counts = np.unique(np.array([item for sublist in corpus for item in sublist]), return_counts=True)\n",
    "    corpus_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    if verbose: \n",
    "        for v, k in sorted(zip(counts, unique), reverse=True): \n",
    "            print('Key is \"{0}\" with count {1}'.format(k, v))\n",
    "\n",
    "    return corpus, corpus_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'this', 'is', 'awesome', 'to', 'be', 'part', 'of']\n",
      "['today', 'is', 'a', 'nice', 'day']\n",
      "['deep', 'learning', 'is', 'cool', 'to', 'do']\n",
      "['have', 'you', 'seen', 'the', 'board', 'today']\n",
      "['are', 'you', 'feeling', 'okay', 'with', 'this']\n",
      "['i', 'like', 'trains', 'very', 'much']\n"
     ]
    }
   ],
   "source": [
    "### Train_iterator\n",
    "def train_loader(path, window_size): \n",
    "    # Build vocabulary\n",
    "    train, train_counts = corpus_counts(path, verbose=False)\n",
    "    vocab_size = len(train_counts)\n",
    "    word_to_idx = {word: i+1 for i, word in enumerate(train_counts.keys())}\n",
    "    word_to_idx['padding'] = 0\n",
    "    \n",
    "    # Run through each sample\n",
    "    word_data, word_targets = [], []\n",
    "    vec_data, vec_targets = [], []\n",
    "    for line in train: \n",
    "        print(line)\n",
    "        \n",
    "        # Add padding corresponding to the size of the window on either side\n",
    "        padding = ['padding']*window_size\n",
    "        line = padding+line+padding\n",
    "        \n",
    "        # Make contexts\n",
    "        sample, target = [], []\n",
    "        for i in range(window_size, len(line) - window_size):\n",
    "            context, c = [], -window_size\n",
    "            while c <= window_size:\n",
    "                if c != 0: \n",
    "                    context.append(line[i+c])\n",
    "                c += 1\n",
    "            target.append(line[i])\n",
    "            sample.append(context)\n",
    "            \n",
    "            # Make context vector\n",
    "            vec_context = make_context_vector(context=context, word_to_ix=word_to_idx)\n",
    "            vec_target = make_context_vector(context=target, word_to_ix=word_to_idx)\n",
    "            vec_data.append(vec_context)\n",
    "            vec_targets.append(vec_target)\n",
    "            \n",
    "        word_data.append(sample)\n",
    "        word_targets.append(target)\n",
    "    \n",
    "    return word_data, word_targets\n",
    "\n",
    "# Call function\n",
    "word_data, word_targets = train_loader(path='data/temp.txt', window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['padding', 'padding', 'world', 'this'], ['padding', 'hello', 'this', 'is'], ['hello', 'world', 'is', 'awesome'], ['world', 'this', 'awesome', 'to'], ['this', 'is', 'to', 'be'], ['is', 'awesome', 'be', 'part'], ['awesome', 'to', 'part', 'of'], ['to', 'be', 'of', 'padding'], ['be', 'part', 'padding', 'padding']] ['hello', 'world', 'this', 'is', 'awesome', 'to', 'be', 'part', 'of'] \n",
      "\n",
      "[['padding', 'padding', 'is', 'a'], ['padding', 'today', 'a', 'nice'], ['today', 'is', 'nice', 'day'], ['is', 'a', 'day', 'padding'], ['a', 'nice', 'padding', 'padding']] ['today', 'is', 'a', 'nice', 'day'] \n",
      "\n",
      "[['padding', 'padding', 'learning', 'is'], ['padding', 'deep', 'is', 'cool'], ['deep', 'learning', 'cool', 'to'], ['learning', 'is', 'to', 'do'], ['is', 'cool', 'do', 'padding'], ['cool', 'to', 'padding', 'padding']] ['deep', 'learning', 'is', 'cool', 'to', 'do'] \n",
      "\n",
      "[['padding', 'padding', 'you', 'seen'], ['padding', 'have', 'seen', 'the'], ['have', 'you', 'the', 'board'], ['you', 'seen', 'board', 'today'], ['seen', 'the', 'today', 'padding'], ['the', 'board', 'padding', 'padding']] ['have', 'you', 'seen', 'the', 'board', 'today'] \n",
      "\n",
      "[['padding', 'padding', 'you', 'feeling'], ['padding', 'are', 'feeling', 'okay'], ['are', 'you', 'okay', 'with'], ['you', 'feeling', 'with', 'this'], ['feeling', 'okay', 'this', 'padding'], ['okay', 'with', 'padding', 'padding']] ['are', 'you', 'feeling', 'okay', 'with', 'this'] \n",
      "\n",
      "[['padding', 'padding', 'like', 'trains'], ['padding', 'i', 'trains', 'very'], ['i', 'like', 'very', 'much'], ['like', 'trains', 'much', 'padding'], ['trains', 'very', 'padding', 'padding']] ['i', 'like', 'trains', 'very', 'much'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample, target in zip(word_data, word_targets): \n",
    "    print(sample, target, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes \n",
    "Consider including padding in loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
